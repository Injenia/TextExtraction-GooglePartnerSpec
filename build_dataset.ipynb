{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dataset building pipeline\n",
    "Riprodurre esattamente gli step utilizzati in predizione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from lib import text_extraction as te\n",
    "from lib import words as wd\n",
    "from lib import embedding as em\n",
    "from gensim.models import Doc2Vec\n",
    "import glob\n",
    "import codecs\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from lib.parallelize import parallelize\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "folders = [\"../atti_costitutivi/\", \"../atti_non_costitutivi/\", \"../verbali_a05/\"]\n",
    "txt_folders =  [\"../atti_costitutivi_txt\", \"../atti_non_costitutivi_txt\", \"../verbali_a05_txt\"]\n",
    "label_names = [\"non_costitutivo\", \"costitutivo\"]\n",
    "folder_labels = [1, 0, 0] \n",
    "gensim_file='../models/gensim_5000_model_with_verb.d2v'\n",
    "permitted_words_file='../dictionaries/first_5000_words_with_verb_cost.json'\n",
    "dataset_filename = '../datasets/embedded_docs_test_v1.p'\n",
    "dataset_filename_word_embedding = '../datasets/word_embedded_docs.p'\n",
    "do_txt_extraction = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gensim_model = Doc2Vec.load(gensim_file)\n",
    "with open(permitted_words_file) as f:\n",
    "    permitted_words = set(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_txts(filenames):\n",
    "    txts = (te.extract_text(f) for f in filenames)\n",
    "    for txt in txts:\n",
    "        if txt != None and len(txt)>0:\n",
    "            yield txt\n",
    "\n",
    "def extract_txts_filenames(filenames):\n",
    "    txts = (te.extract_text(f) for f in filenames)\n",
    "    for filename,txt in zip(filenames,txts):\n",
    "        if (txt != None and len(txt)>0):\n",
    "            yield filename, txt\n",
    "\n",
    "def dataset_generator_from_folders(folders, folder_labels, gensim_model, permitted_words, extract_text=False):\n",
    "    for folder, label in zip(folders, folder_labels):\n",
    "        filenames = glob.glob(folder+'/*')\n",
    "        if extract_text:\n",
    "            txts = extract_txts(filenames)\n",
    "        else:\n",
    "            txts = (open(f).read() for f in filenames)\n",
    "        splitted_txts = (wd.tokenize_doc(txt) for txt in txts)\n",
    "        embedded_txts = (em.embed_document(gensim_model, doc, permitted_words) for doc in splitted_txts)\n",
    "        for i, e in enumerate(embedded_txts):\n",
    "            print(i)\n",
    "            yield (e, label)   \n",
    "\n",
    "def dataset_generator_files_word_embedding(files_lists, file_list_labels, reduced_dictionary):\n",
    "    d = em.DictionaryMapper(reduced_dictionary)\n",
    "    for filenames, label in zip(files_lists, file_list_labels):\n",
    "        txts = (open(f).read() for f in filenames)\n",
    "        tokenized_txts = (wd.word_tokenize_replace(txt) for txt in txts)\n",
    "        for doc in d.map_to_ints(tokenized_txts):\n",
    "            yield (doc, label)   \n",
    "                   \n",
    "def extract_txts_to_folders(folders, out_folders):\n",
    "    for folder, out_folder in zip(folders, out_folders):\n",
    "        filenames = glob.glob(folder+'/*')\n",
    "        for filename, txt in extract_txts_filenames(filenames):\n",
    "            print(filename)\n",
    "            with open(os.path.join(out_folder, os.path.basename(filename))) as o:\n",
    "                o.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_dataset = list(dataset_generator_from_folders(txt_folders, folder_labels, gensim_model, \n",
    "                                                   permitted_words, extract_text= False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#balancing\n",
    "non_costitutivi = [(d,l) for d,l in full_dataset if l == 0]\n",
    "lnc = len(non_costitutivi)\n",
    "costitutivi = [(d,l) for d,l in full_dataset if l == 1]\n",
    "lc = len(costitutivi)\n",
    "minlen = min([lc,lnc])\n",
    "balanced_dataset = costitutivi[:minlen] + non_costitutivi[:minlen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "docs = [d for d,l in balanced_dataset]\n",
    "labels = [l for d,l in balanced_dataset]\n",
    "with open(dataset_filename, \"w\") as fout:\n",
    "        pickle.dump([docs, labels], fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if do_txt_extraction:\n",
    "    extract_txts_to_folders(folders, txt_out_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build dataset for word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464.042852163\n"
     ]
    }
   ],
   "source": [
    "maxwords = 9998\n",
    "start = time.time()\n",
    "cost_texts = (open(filename).read() for filename in glob.glob(\"../atti_costitutivi_txt/*\"))\n",
    "tokenized_costs = [wd.word_tokenize_replace(txt) for txt in cost_texts]\n",
    "print(time.time()-start)\n",
    "dm = em.DictionaryMapper()\n",
    "dm.fit_texts(tokenized_costs, maxwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"reduced_dictionary_cost.json\", 'w') as o:\n",
    "    json.dump(dm.reduced_dictionary, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Start from here if you already have the dictionary\n",
    "with open(\"reduced_dictionary_cost.json\") as f:\n",
    "    reduced_dictionary = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "non_cost_filenames = glob.glob(\"../atti_non_costitutivi_txt/*\")+ glob.glob(\"../verbali_a05_txt/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cost_filenames = glob.glob(\"../atti_costitutivi_txt/*\")[:len(non_cost_filenames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_dataset = list(dataset_generator_files_word_embedding([cost_filenames, non_cost_filenames],\n",
    "                                                           [1,0], reduced_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "balanced_dataset = full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "docs = [d for d,l in balanced_dataset]\n",
    "labels = [l for d,l in balanced_dataset]\n",
    "with open(dataset_filename_word_embedding, \"w\") as fout:\n",
    "        pickle.dump([docs, labels], fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
