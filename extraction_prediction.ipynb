{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import sequence\n",
    "from lib import words as wd\n",
    "from lib import embedding as em\n",
    "from lib import text_extraction as te\n",
    "from lib import predict_pdf as pp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def is_scadenza(s):\n",
    "    return re.match(r'.*primo\\s+?esercizio.*', s) != None\n",
    "\n",
    "def post_process_prediction(sents, y_pred, neutral, milen=10):\n",
    "    y_post = list(y_pred)\n",
    "    for i in range(1,len(y_pred)-1):\n",
    "        y_post[i] = y_post[i+1] if y_post[i] == neutral and y_post[i-1] == y_post[i+1] and len(sents[i])>=milen else y_post[i]\n",
    "    return y_post\n",
    "\n",
    "class PartsExtraction(object):\n",
    "    def __init__(self, keras_model, reduced_dict, labels, maxlen = 100):\n",
    "        self._model = keras_model\n",
    "        self._reduced_dict = reduced_dict\n",
    "        self._labels = labels\n",
    "        self._maxlen = maxlen\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_from_files(keras_model_filename, keras_weights_filename,\n",
    "                        reduced_dict_filename, labels=['poteri', 'assemblea', 'clausola', 'non_riconducibile', 'scadenza']):\n",
    "        with open(keras_model_filename) as f:\n",
    "            km = model_from_json(f.read())\n",
    "        km.load_weights(keras_weights_filename)\n",
    "        with open(reduced_dict_filename) as f:\n",
    "            rd = json.load(f)\n",
    "        return PartsExtraction(km, rd, labels)\n",
    "    \n",
    "    def _int_sentences(self, sentences):\n",
    "        splitted_sentences = wd.tokenize_sentences(sentences, min_words=1)\n",
    "        permitted_words = self._reduced_dict.keys()\n",
    "        reduced_sentences = list(em.reduce_dictionary(splitted_sentences, permitted_words, min_words=1))\n",
    "        return [[self._reduced_dict[w] for w in sent] for sent in reduced_sentences]\n",
    "    \n",
    "    def extract_parts_prob(self, sentences):\n",
    "        int_sentences = self._int_sentences(sentences)\n",
    "        padded_data = sequence.pad_sequences(int_sentences, self._maxlen, padding=\"pre\", truncating=\"post\", value=0, dtype='uint32')\n",
    "        return self._model.predict(padded_data)\n",
    "    \n",
    "    def extract_parts(self, sentences, post_process=False, probas = []):\n",
    "        if len(probas)==0:\n",
    "            probas = self.extract_parts_prob(sentences) \n",
    "        predictions = probas.argmax(axis=-1)\n",
    "        for i in range(len(predictions)):\n",
    "            if is_scadenza(sentences[i]):\n",
    "                predictions[i] = self._labels.index('scadenza')\n",
    "                break\n",
    "        if post_process:\n",
    "            preds = post_process_prediction(sentences, predictions, self._labels.index('non_riconducibile'))\n",
    "        else:\n",
    "            preds = predictions\n",
    "        return [self._labels[i] for i in preds]\n",
    "    \n",
    "    def extract_parts_dict(self, sentences, predictions=None):\n",
    "        predictions = self.extract_parts(sentences) if predictions == None else predictions\n",
    "        df = pd.DataFrame({'sentence':sentences,'prediction':predictions})\n",
    "        pivoted = df.pivot(columns='prediction', values='sentence')\n",
    "        return {k:list(filter(None, pivoted[k])) for k in labels}\n",
    "    \n",
    "    def extract_parts_dict_indexes(self, predictions):\n",
    "        df = pd.DataFrame({'sentence':list(range(len(predictions))),'prediction':predictions})\n",
    "        pivoted = df.pivot(columns='prediction', values='sentence')\n",
    "        return {k:[int(i) for i in filter(lambda x: x==x, pivoted[k])] for k in labels} #nan != nan \n",
    "    \n",
    "def is_valid_nl(txt, threshold=0.075):\n",
    "    return txt.count('\\n')/len(txt)<=threshold\n",
    "\n",
    "def labels_probas_dict(labels, p):\n",
    "    return {l:pr for l,pr in zip(labels, p)}\n",
    "\n",
    "def sentences_probas_dict(sentences, probas):\n",
    "    return [{'frase':s,'prob':labels_probas_dict(labels[:-1], p)} for s,p in zip(sentences, probas)]\n",
    "\n",
    "def sentences_probas(sentences, probas):\n",
    "    return [{'frase':s,'prob':list(p)} for s,p in zip(sentences, probas)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = ['poteri', 'assemblea', 'clausola', 'non_riconducibile', 'scadenza']\n",
    "\n",
    "pe = PartsExtraction.load_from_files('models/extraction_model_30_all.json',\n",
    "                                     'models/extraction_weights_30_all.h5',\n",
    "                                     'first_5000_words_extraction.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "document = '../extraction/esempi_descrizioni/5115612230001.pdf'#'../extraction/files_to_label/4907913200001.pdf'\n",
    "\n",
    "txt = te.extract_text(document, do_ocr=False, pages=-1)\n",
    "sentences = wd.sentences_doc(txt, rep=' ', newline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = pe.extract_parts(sentences, post_process=False)\n",
    "list(enumerate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = pe.extract_parts(sentences, post_process=True)\n",
    "list(enumerate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i,(p,s) in enumerate(zip(predictions, sentences)):\n",
    "    print('\\n[{}] PREDICTION: {}\\n'.format(i, p))\n",
    "    print(s.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df = pd.DataFrame({'sentence':sentences,'prediction':predictions})\n",
    "#pivoted = df.pivot(columns='prediction', values='sentence')\n",
    "#d = {k:list(filter(None, pivoted[k])) for k in labels if k != 'non_riconducibile'}\n",
    "\n",
    "d = pe.extract_parts_dict(sentences, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k,v in d.items():\n",
    "    if k != 'non_riconducibile':\n",
    "        print('\\n\\n'+k.upper().center(127,'.')+'\\n')\n",
    "        print('.\\n\\n'.join([s.strip() for s in v]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probas = pe.extract_parts_prob(sentences)\n",
    "predictions = pe.extract_parts(sentences, post_process=True, probas=probas)\n",
    "dict_indexes = pe.extract_parts_dict_indexes(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_sentences = {k:[sentences[i] for i in dict_indexes[k]] for k in dict_indexes.keys()}\n",
    "dict_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k,v in dict_sentences.items():\n",
    "    if k != 'non_riconducibile':\n",
    "        print('\\n\\n'+k.upper().center(127,'.')+'\\n')\n",
    "        print('.\\n\\n'.join([s.strip() for s in v]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold sensato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_thres_nl(txt):\n",
    "    t = 1\n",
    "    while is_valid_nl(txt, t):\n",
    "        t -= 0.001\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt_brutto = open('../extraction/esempi_extracted/5116982390001.txt').read()\n",
    "print(get_thres_nl(txt_brutto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,f in enumerate(glob.glob('../atti_costitutivi_txt/*')[:100]):\n",
    "    text = open(f).read()\n",
    "    print(f, get_thres_nl(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thress = [get_thres_nl(open(f).read()) for f in  glob.glob('../atti_costitutivi_txt/*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(thress, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for e in pe.extract_parts_prob(sentences):\n",
    "    print('{:4f}\\t{:4f}\\t{:4f}\\t{:4f}'.format(*e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences_probas(sentences, probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pe.extract_parts_prob(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = pp.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99852413"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.predict_document_str(txt, **models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
