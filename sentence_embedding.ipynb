{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Embedding delle Frasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import operator\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_dictionary(sentences):\n",
    "    d = dict()\n",
    "    index = 0\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if not word in d:\n",
    "                d[word] = index\n",
    "                index += 1\n",
    "    return d\n",
    "\n",
    "def word_counts(sentences):\n",
    "    d = dict()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if not word in d:\n",
    "                d[word] = 1\n",
    "            else:\n",
    "                d[word] += 1\n",
    "    return d\n",
    "\n",
    "def rev_list(d):\n",
    "    rd = [None]*len(d)\n",
    "    for w,i in d.items():\n",
    "        rd[i] = w\n",
    "    return rd\n",
    "\n",
    "def rev_dict(d):\n",
    "    rd = dict()\n",
    "    for w,i in d.items():\n",
    "        rd[i] = w\n",
    "    return rd\n",
    "\n",
    "def words_to_ints(words,d):\n",
    "    return [d[w] for w in d]\n",
    "\n",
    "def sentences_to_int_lists(sentences,d):\n",
    "    return [words_to_ints(words) for words in sentences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Creazione del dataset come sottoinsieme bilanciato dei documenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "csv_filename = '../atti.csv'\n",
    "model_filename = 'gensim_model.d2v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7246"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_nc = len(df.loc[df['label'] == 'non_costitutivo'].groupby('filename'))\n",
    "size_nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped = df.loc[df['label'] == 'costitutivo'].groupby(df[\"filename\"])\n",
    "dfs = [g[1] for g in list(grouped)[:size_nc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped_nc = df.loc[df['label'] == 'non_costitutivo'].groupby(df[\"filename\"])\n",
    "dfs_nc = [g[1] for g in list(grouped_nc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_balanced = pd.concat(dfs + dfs_nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_sentences = df_balanced['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Creazione degli embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    '''Pass the pandas column'''\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "    def __iter__(self):\n",
    "        i = 0\n",
    "        for line in sentences:\n",
    "            yield LabeledSentence(utils.to_unicode(line).split(), ['SENT_%s' % i])\n",
    "            i += 1\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences\n",
    "            \n",
    "def iter_sentences(sents):\n",
    "    i = 0\n",
    "    for line in sents:\n",
    "        yield LabeledSentence(line, ['SENT_%s' % i])\n",
    "        i += 1\n",
    "\n",
    "def randomly(seq):\n",
    "    shuffled = list(seq)\n",
    "    random.shuffle(shuffled)\n",
    "    return iter(shuffled)\n",
    "\n",
    "#sentences = iter_sentences(pd_sentences) #LabeledLineSentence(pd_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Modello dell'embedding\n",
    "\n",
    "def build_embedding(sentences):\n",
    "    if os.path.exists(model_filename):\n",
    "        model = Doc2Vec.load(model_filename)\n",
    "    else:\n",
    "        model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-5, negative=5, workers=2)\n",
    "        model.build_vocab(sentences)\n",
    "        print 'Vocabulary built'\n",
    "        #for epoch in range(5):\n",
    "        #    print 'Epoch', epoch\n",
    "        model.train(sentences, model.corpus_count, epochs = 5)\n",
    "        model.save(model_filename)\n",
    "        print 'Model saved'\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "d = build_dictionary(s.split() for s in pd_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138822"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def first_n_words(dictionary, n):\n",
    "    rd = rev_dict(d)\n",
    "    wc = word_counts(s.split() for s in pd_sentences)\n",
    "    sorted_wc = sorted(wc.items(), key=operator.itemgetter(1))\n",
    "    return set(reversed([x[0] for x in sorted_wc[-n:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "first_10000_words = first_n_words(d, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def substitute_word(word, permitted_words, unknown = 'UNK'):\n",
    "    return word if word in permitted_words else unknown\n",
    "\n",
    "def reduced_sentence(sentence, permitted_words):\n",
    "    return [substitute_word(word, permitted_words) for word in sentence]\n",
    "\n",
    "def reduce_dictionary(sentences, permitted_words, min_words=2):\n",
    "    for sentence in sentences:\n",
    "        new_sentence = reduced_sentence(sentence, permitted_words)\n",
    "        if len(new_sentence) >= min_words:\n",
    "            yield new_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "filtered_sentences = reduce_dictionary((s.split() for s in pd_sentences), first_10000_words)\n",
    "filtered_sentences_list = list(filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary built\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Esecuzione del modello\n",
    "\n",
    "model = build_embedding(list(iter_sentences(filtered_sentences_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('presente', 0.8062084913253784),\n",
       " ('apportare', 0.7802978754043579),\n",
       " ('soppressioni', 0.7477972507476807),\n",
       " ('allega', 0.7333101034164429),\n",
       " ('aggiunte', 0.7322391271591187),\n",
       " ('statuto', 0.731306791305542),\n",
       " ('integrante', 0.7226213216781616),\n",
       " ('regolata', 0.7120342254638672),\n",
       " ('forza', 0.7065362930297852),\n",
       " ('allegano', 0.7060737609863281)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('atto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sentence_vector(model, sentence, permitted_words):\n",
    "    return model.infer_vector(reduced_sentence(sentence.split(' '), permitted_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00877847,  0.08421552, -0.00934239, -0.06026119, -0.0486962 ,\n",
       "       -0.10909535,  0.01427933,  0.0382507 ,  0.06857342, -0.05685881,\n",
       "       -0.01122126,  0.05149804, -0.07070133,  0.01299185,  0.08797149,\n",
       "       -0.02599948,  0.00463746, -0.02856188,  0.00580665,  0.01292427,\n",
       "        0.0679419 ,  0.0067199 ,  0.00769502,  0.07473454,  0.02272988,\n",
       "       -0.02177911, -0.05640026,  0.08370669,  0.01138843, -0.07007347,\n",
       "       -0.08425567, -0.00059223, -0.0390888 ,  0.0140768 , -0.11788081,\n",
       "        0.01243659, -0.06523187,  0.02117974, -0.00639699, -0.00492997,\n",
       "        0.01472021, -0.0201715 ,  0.00113072,  0.01332173, -0.02123491,\n",
       "       -0.01164006, -0.00858516,  0.06819962, -0.02509951,  0.02500732,\n",
       "        0.0713005 , -0.09450735, -0.04908381,  0.03631534, -0.08748867,\n",
       "        0.06206969,  0.00515663,  0.02944721,  0.0006274 ,  0.0327379 ,\n",
       "        0.0844673 , -0.06255493, -0.03983356, -0.07354014, -0.02518529,\n",
       "        0.05504263,  0.06384774, -0.05127484, -0.00868062,  0.00413347,\n",
       "       -0.01247259, -0.08092891,  0.00220306, -0.02664082,  0.01037841,\n",
       "       -0.04024276, -0.00314954, -0.08573674, -0.01097692, -0.00526486,\n",
       "       -0.01445304, -0.03740054,  0.10652575, -0.03080152,  0.00664233,\n",
       "        0.01077657,  0.00609271, -0.04949525,  0.05309283, -0.00489469,\n",
       "        0.00615434,  0.13082555,  0.04873852, -0.01660067,  0.02400776,\n",
       "        0.01225106, -0.00162087,  0.05861737, -0.01292208, -0.06704711], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Esempio di sentence vector\n",
    "\n",
    "sv = sentence_vector(model, pd_sentences[551068], first_10000_words)\n",
    "sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Costruzione del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(model, df, permitted_words):\n",
    "    #embeddings = np.array([sentence_vector(model, sentence, permitted_words) for sentence in sentences])\n",
    "    filename = \"\"\n",
    "    docs = []\n",
    "    labels = []\n",
    "    curdoc = []                  # lista delle frasi del documento corrente\n",
    "    for i in xrange(len(df)):\n",
    "        row = df.iloc[i] \n",
    "        if filename == \"\":\n",
    "            filename = row[\"filename\"]\n",
    "            labels.append(row[\"label\"])\n",
    "            \n",
    "        embedding = sentence_vector(model, row['sentence'], permitted_words)\n",
    "        if filename == row[\"filename\"]:\n",
    "            curdoc.append(embedding)\n",
    "        else:\n",
    "            print \"%s with len: %d\" % (filename, len(curdoc))\n",
    "            docs.append(curdoc)\n",
    "            curdoc = [embedding]\n",
    "            labels.append(row[\"label\"])\n",
    "            filename = row['filename']\n",
    "    if len(curdoc)>0:\n",
    "        docs.append(curdoc)\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs, labels = build_dataset(model, df_balanced, first_10000_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "label_map = {'costitutivo':1, 'non_costitutivo':0}\n",
    "labels_n = [label_map[l] for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"embedded_docs.p\", \"w\") as fout:\n",
    "    pickle.dump([docs, labels_n], fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
